\documentclass{article}
\usepackage{graphicx, amsfonts, amsmath, amsthm}
\usepackage{natbib}
\bibliographystyle{unsrtnat}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{bbm}
\newcommand{\argmin}{\text{argmin}}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\veps}{\varepsilon}
\newcommand{\bPp}{\bPartial}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\wt}{\widetilde}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bQ}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calP}{\mathcal{P}}

\setlength{\textheight}{8.5in}
\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\parindent}{0pt}

\title{Applied Qual Study Guide}
\author{Ting Chen}
\date{\today}

\begin{document}

\maketitle

\section{Statistical Models}
For these models, the following questions are to be answered:
\begin{itemize}
  \item Model assumptions
  \item Estimation. Usually there are more than one way to estimate model parameters, each of which arises from their own context and requires different assumptions
  \item Inference questions: Frequentist distribution, confidence intervals, posterior-distribution based uncertainty measures, etc.
  \item Model diagnosis and refinement; robustness of estimation and inference to assumptions.
  \item Model selection/regularization and their computation
\end{itemize}

\subsection{Linear model}
BLUE
\begin{itemize}
  \item Best (least variance)
  \item Linear
  \item Unbiased
  \item Estimator
\end{itemize}
Gauss-Markov Theorem - no better linear unbiased estimator exists.\newline \\
\textbf{Proof:}\\
Consider linear estimate of $\hat{\beta} = \sum_{i=1}^{n}a_i(y_i - \bar{y})$. Then the bias is
$$\mathbb{E}_\varepsilon[\hat{\beta}] = \mathbb{E}_\varepsilon\left[\sum_{i=1}^{n}a_i(\alpha + \beta x_i + \varepsilon_i - \bar{y})\right] = \mathbb{E}_\varepsilon\left[\sum_{i=1}^{n}a_i(\bar{y} - \beta \bar{x} + \beta x_i + \varepsilon_i - \bar{y})\right] = \beta \sum_{i=1}^{n}a_i(x_i - \bar{x})$$
and the variance is
\begin{align*}
  \mathtt{Var}_\varepsilon[\hat{\beta}] &= \mathtt{Var}_\varepsilon[\hat{\beta} - \beta] \\
  &= \mathtt{Var}_\varepsilon\left[\sum_{i=1}^{n}a_i(y_i - \bar{y}) - \beta\right] \\
  &= \mathtt{Var}_\varepsilon\left[\sum_{i=1}^{n}a_i(\beta(x_i - \bar{x}) + (\varepsilon_i - \bar{\varepsilon})) - \beta\right]\\
  &= \mathtt{Var}_\varepsilon\left[\beta\sum_{i=1}^{n}a_i(x_i - \bar{x}) + \sum_{i=1}^{n}a_i(\varepsilon_i - \bar{\varepsilon}) - \beta\right]\\
  &= \mathtt{Var}_\varepsilon\left[\sum_{i=1}^{n}a_i(\varepsilon_i - \bar{\varepsilon})\right]\\
  &= \mathtt{Var}_\varepsilon\left[\sum_{i=1}^{n}\varepsilon_i(a_i - \bar{a})\right]\\
  &= \sigma^2_\varepsilon \sum_{i=1}^{n}(a_i - \bar{a})^2
\end{align*}
To show the OLS estimates are BLUE, we then solve the constrained minimization problem via Lagrangian multipliers.
\begin{align*}
  \min_{a_1, \ldots, a_n} \quad &\sum_{i=1}^{n}(a_i - \bar{a})^2 = \sum_{i=1}^{n}a_i^2 - n\bar{a}\\
  \textrm{s.t.} \quad & \sum_{i=1}^{n}a_i(x_i - \bar{x}) = 1
\end{align*}
Taking the derivative wrt to $a_i$ and plugging back into the constraint to get a value for $\lambda$ yields
$$a_i = \frac{x_i - \bar{x}}{\sum_{i=1}^{n}(x_i - \bar{x})^2} $$
as desired.
\subsubsection{Model assumptions}
\begin{enumerate}
  \item Gaussian errors - not really needed, can be dropped if sample size is large
  \item Homoskedasticity
  \item Additive and linear relationship
  \item errors are i.i.d. - not really needed, just uncorrelated and homoskedastic errors
  \item zero mean errors
    % \item Independent variables are not highly correlated (no multicollinearity)
\end{enumerate}

When $x$ and $y$ are standardized, the regression line always has slope less than 1.
Thus, when $x$ is 1 standard deviation above the mean, the predicted value of $y$ is somewhere between 0 and 1 standard deviations above the mean. This phenomenon in linear models—that y is predicted to be closer to the mean (in standard-deviation units) than x—is called regression to the mean and occurs in many vivid contexts.
\subsubsection{Estimation}
\begin{enumerate}
  \item (O)Least Squares, directly, maximum likelihood estimate:
    $$ y_i = \alpha + \beta x_i + \varepsilon_i $$ for $i=1,\ldots, n$.
    Want to minimize SSE
    $$ SSE(\alpha, \beta) = \sum_{i=1}^{n} (y_i - \hat{y})^2 = \sum_{i=1}^{n} (y_i - (\alpha + \beta x_i))^2$$
    Taking the derivatives and solving, we get
    $$ \hat{\beta} = \frac{\sum_{i=1}^{n}(y_i - \bar{y})(x_i - \bar{x})}{\sum_{i=1}^{n}(x_i - \bar{x})^2} = \frac{\mathtt{cov}(x, y)}{\mathtt{var}(x)} = \rho_{x, y} \cdot \frac{s_y}{s_x}$$
    $$ \hat{\alpha} = \bar{y} - \hat{\beta} \bar{x}$$

    Where $s_y = \sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}$,  $s_x = \sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2}$ (Note: This form of $\alpha$ implies that the regression line must pass through ($\bar{x}$, $\bar{y}$)), and
    $$\rho_{x, y} = \frac{\sum_{i=1}^{n}(y_i - \bar{y})(x_i - \bar{x})}{s_x s_y} = \frac{\sum_{i=1}^{n}x_iy_i - n\bar{x}\bar{y}}{\sqrt{\sum_{i=1}^{n}x_i^2 - n \bar{x}^2}\sqrt{\sum_{i=1}^{n}y_i^2 - n \bar{y}^2}}$$
    You get regression to the mean if $\rho_{x, y} < 1$. Some useful properties include
    \begin{enumerate}
      \item $\sum_{i=1}^{n} \hat{\epsilon}_i= 0$ $\leftarrow$ take derivative of SSE wrt $\alpha$
      \item $\sum_{i=1}^{n} x_i\hat{\epsilon}_i= 0$ $\leftarrow$ take derivative of SSE wrt $\beta$
      \item $\sum_{i=1}^{n} \hat{y}_i\hat{\epsilon}_i= 0$ $\leftarrow$ consequence of the above
    \end{enumerate}
    which is a consequence of the first order conditions.
    \\ \\
    Note $$ SSE = \mathbb{E}[(Y - \mathbb{E}[Y|X])^2] + \mathbb{E}[(\mathbb{E}(Y|X) - (a + bX))^2] $$
    (Cross term drops because noise is independent), hence least squares estimate is best linear approximation to $\mathbb{E}[Y|X = x]$.
    \\ \\
    Thought experiment assuming $X$ is standard Gaussian, can show via Stein's identity that by minimizing MSE, we are estimating slope of regression function (averaged derivative under Gaussian).
    \\ \\
    Also note that the error variance is $$ \hat{\sigma}^2 = \frac{1}{n-2} \sum_{i}r_i^2 $$
    where $r_i := y_i - \hat{y}_i = y_i - (\hat{\alpha} - \hat{\beta}x_i)$.
  \item Gradient descent/Newton-Raphson if more params than observations or multicollinearity, can go for regularization to solve this too,
  \item Moore-Penrose pseudo-inverse
  \item Bayesian methods (MAP, MCMC, VI, etc.)
\end{enumerate}
\subsubsection{Inference questions}
\textbf{Sampling distributions}\\
The sampling distribution of the estimates slope, intercept and residual variance, conditional on $x_1, \ldots, x_n$, are as follows:
$$ \hat{\beta} = \beta + \frac{\sum_{i=1}^{n}(x_i - \bar{x})(\epsilon_i - \bar{\epsilon})}{\sum_{i=1}^{n}(x_i - \bar{x})^2} \sim \mathcal{N}\left(\beta, \frac{\sigma^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\right)$$

note to derive the above we use the fact that the sum of deviations from the mean is always zero, i.e. $\sum_{i=1}^{n} (x_i - \bar{x}) = 0$. \\

Since $\bar{y} \perp \hat{\beta}\bar{x}$,
$$ \hat{\alpha} = \bar{y} - \hat{\beta}\bar{x} \sim \mathcal{N}\left(\alpha, \sigma^2\left[ \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\right]\right) $$
\\

Finally
$$ \hat{\sigma}^2 \sim \sigma^2 \chi^2_{n-2}/(n-2) $$
and note that $(\hat{\alpha}, \hat{\beta}) \perp \hat{\sigma}^2$. \\

\textbf{Proof}: Distribution of Residual Variance using Idempotent Matrix $\chi^2$ Theorem \\
Consider the linear regression model:
\[
  Y = X\beta + \varepsilon, \quad \varepsilon \sim N_n(0, \sigma_0^2 I).
\]

The least squares estimator is:
\[
  \hat{Y} = HY, \quad \text{where } H = X(X^\top X)^{-1}X^\top.
\]

Then the residual vector is:
\[
  r = Y - \hat{Y} = (I - H)Y = (I - H)\varepsilon,
\]
because \( HX\beta = X\beta \).

The residual sum of squares (RSS) is:
\[
  \text{RSS} = r^\top r = \varepsilon^\top (I - H)\varepsilon.
\]

Now apply the \textbf{idempotent matrix chi-square theorem} \href{https://www.statlect.com/probability-distributions/normal-distribution-quadratic-forms}{see link here}:

\begin{itemize}
  \item \( \varepsilon \sim N_n(0, \sigma_0^2 I) \)
  \item \( I - H \) is symmetric and idempotent
  \item \( \text{rank}(I - H) = n - \text{rank}(H) = n - p \), where \( p = \text{number of parameters in } \beta \)
\end{itemize}

In simple linear regression, \( p = 2 \), so:
\[
  \frac{1}{\sigma_0^2}\varepsilon^\top (I - H)\varepsilon \sim \chi^2_{n - 2}.
\]

Hence,
\[
  \hat{\sigma}^2 = \frac{1}{n - 2} \varepsilon^\top (I - H)\varepsilon \sim \frac{\sigma_0^2}{n - 2} \chi^2_{n - 2}.
\]\\

\textbf{Confidence intervals on coefficients with $t$-dist}\\
Under $H_0: \beta = 0$
\[ \frac{\hat{\beta}}{\frac{\hat{\sigma}}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2}}} \sim t_{n-2} \]
Under $H_0: \alpha = 0$
\[ \frac{\hat{\alpha}}{\hat{\sigma}\sqrt{\left[ \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\right]}} \sim t_{n-2} \]

\textbf{ANOVA (analysis of variance)}\\
\[ \underbrace{\sum_{i=1}^{n} (y_i - \bar{y})^2}_{SST} = \underbrace{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}_{SSE} + \underbrace{\sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2}_{SSR}\]
Coefficient of determination:
\[ R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST} \]
Note for OLS $R^2 = \rho^2_{X, Y}$ \\
\textbf{Proof:}
\[ \rho^2_{X, Y} =  \frac{(\sum_{i=1}^{n}(y_i - \bar{y})(x_i - \bar{x}))^2}{(\sum_{i=1}^{n} (y_i - \bar{y})^2)(\sum_{i=1}^{n} (x_i - \bar{x})^2)}\]
and
\begin{align*}
  R^2 &= \frac{SSR}{SST}\\
  &= \frac{\sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} \\
  &= \frac{\sum_{i=1}^{n} (\hat{\alpha} + \hat{\beta}x_i - \bar{y})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} \\
  &= \frac{\hat{\beta}^2 \sum_{i=1}^{n} (x_i - \bar{x})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} \\
  &= \frac{(\sum_{i=1}^{n}(y_i - \bar{y})(x_i - \bar{x}))^2}{(\sum_{i=1}^{n} (y_i - \bar{y})^2)(\sum_{i=1}^{n} (x_i - \bar{x})^2)}
\end{align*}
\\
Residual standard error (RSE): \[ RSE = \sqrt{\frac{SSR}{n - p - 1}} \]

\textbf{Compare models with $F$-test}\\
Measure goodness of fit of your model. Using facts that $SSE \perp SSR$,
$SSE \sim \sigma^2 \chi^2_{n-2}$, $SSR \sim \sigma^2 \chi^2_{1}$ then
$F$-test for $H_0: \beta=0$ is
\[ F = \frac{SSR}{SSE/(n-2)} \sim F_{1, n-2}\]
Note that the $p$-value for the $F$-test and $t$-test for $\beta$ are equal in the simple linear regression case.\\

% \textbf{Test variance with $\chi^2$-test}\\

\textbf{Prediction intervals}\\
For new data $x_{\text{new}}$, our estimate $\hat{y}_{\text{new}} = \hat{\alpha} + x_{\text{new}} \hat{\beta}$
is unbiased. The variance is
\begin{align*}
  \mathtt{Var}(\hat{y}_\text{new} | x, x_{\text{new}}) &= \mathtt{Var}(\hat{\alpha} | x)  + x_\text{new}^2 \mathtt{Var}(\hat{\beta} | x, x_{\text{new}})  + 2 x_{\text{new}} \mathtt{Cov}(\hat{\alpha}, \hat{\beta})\\
  &= \sigma^2 \left(\frac{1}{n} + \frac{(x_\text{new} - \bar{x})^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\right)
\end{align*}
where $\mathtt{Cov}(\hat{\alpha}, \hat{\beta}) = \frac{-\sigma^2\bar{x}}{\sum_{i=1}^{n}(x_i - \bar{x})^2}$. For proof of this
consider the following:

\begin{align*}
  \mathtt{Cov}(\hat{\alpha}, \hat{\beta}) &= \mathtt{Cov}(\bar{y} - \hat{\beta}\bar{x}, \hat{\beta})\\
  &= \mathtt{Cov}(\bar{y}, \hat{\beta}) -  \mathtt{Cov}(\hat{\beta} \bar{x}, \hat{\beta})\\
  &= \mathtt{Cov}\left(\frac{1}{n} \sum_{i=1}^{n} y_i, \frac{\sum_{i=1}^{n}y_i (x_i - \bar{x})}{\sum_{j=1}^{n} (x_j - \bar{x})^2} \right) -  \bar{x} \mathtt{Var}(\hat{\beta}) \\
  &= \frac{\sum_{i=1}^{n}\sigma^2 (x_i - \bar{x})}{n\sum_{j=1}^{n} (x_j - \bar{x})^2} -  \frac{\sigma^2 \bar{x}}{\sum_{j=1}^{n} (x_j - \bar{x})^2} \ \ \ \text{(See Lemma 11.3.2. from Casella and Berger)} \\
  &= 0 -  \frac{\sigma^2 \bar{x}}{\sum_{j=1}^{n} (x_j - \bar{x})^2}
\end{align*}

Hence
\[ \hat{y}_\text{new} \sim \mathcal{N}\left(\alpha + x_\text{new} \cdot \beta, \sigma^2 \left( \frac{1}{n} + \frac{(x_\text{new} - \bar{x})^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\right)\right) \]
and it follows that a CI to use would be
\[\hat{\alpha} + x_\text{new} \cdot \hat{\beta} \pm t_{n-2, 1 - \alpha/2} \cdot \hat{\sigma} \sqrt{\frac{1}{n} + \frac{(x_\text{new} - \bar{x})^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}}\]
However we're typically interested in an interval for the actual observations rather than on the mean. Hence
\begin{align*}
  \mathtt{Var}(y_\text{new} - \hat{y}_\text{new}) &= \mathtt{Var}(y_\text{new}) + \mathtt{Var}(\hat{y}_\text{new}) \\
  &= \sigma^2 \left( 1+ \frac{1}{n} + \frac{(x_\text{new} - \bar{x})^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\right)
\end{align*}
hence the CI we do use is
\[\hat{\alpha} + x_\text{new} \cdot \hat{\beta} \pm t_{n-2, 1 - \alpha/2} \cdot \hat{\sigma} \sqrt{1 + \frac{1}{n} + \frac{(x_\text{new} - \bar{x})^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}}\]

\textbf{Relaxing assumptions and their impacts on CIs:}
\begin{enumerate}
  \item Normality
    \begin{itemize}
      \item Check with Q-Q plot of residuals
      \item Can be dropped with large sample sizes as by (Lindeberg-Feller) CLT note that
        \[\hat{\beta} \xrightarrow{d} \mathcal{N}\left(\beta, \frac{\sigma^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\right) \]
        and
        \[\hat{\alpha} \xrightarrow{d} \mathcal{N}\left(\alpha, \sigma^2 \left(\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2} \right) \right) \]
        However in this regime $\hat{\alpha}$ and $\hat{\beta}$ are \textbf{\textit{not independent}} of $\hat{\sigma}^2$ and hence we must use Slutsky's to justify
        using normal quantiles in our confidence intervals (the side effect here is also that the use of  $t$-distribution quantiles no longer become valid).
    \end{itemize}
  \item Linearity
    \begin{itemize}
      \item Check with residual vs. fitted value plots
      \item When there is nonlinearity and $\alpha + \beta X$ are still the best linear approximation, then point estimates and standard errors are still valid but the interpretations are different (this is just the best linear approximation).
        Consider \[\mathbb{E}[Y|X] = \alpha + \beta X + \delta(X)\]
        If $\alpha + \beta X$ is the best linear approximation then (assuming $X$ is random)
        \[\mathbb{E}[\delta(X)] = \mathbb{E}[X \delta(X)] = 0\]
        ($\alpha$ is best intercept and $\beta$ is the best linear term). In which case we still have
        \[\hat{\beta} \xrightarrow{d} \mathcal{N}\left(\beta, \frac{\sigma^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\right) \]
        and
        \[\hat{\alpha} \xrightarrow{d} \mathcal{N}\left(\alpha, \sigma^2 \left(\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2} \right) \right) \]
        where $\sigma^2 = \sigma^2_0 + \mathbb{E}[\delta(X)^2]$
        Finally, note that
        \[\hat{\sigma}^2 \xrightarrow{p} \sigma^2 = \sigma^2_0 + \mathbb{E}[\delta(X)^2] > \sigma^2_0\]
    \end{itemize}
  \item Homoskedasticity
    \begin{itemize}
      \item Check with residual vs. fitted value plots
      \item If we drop this, our point estimates remain valid, but the standard errors and inferences need to be adjusted.
        Consider $\mathtt{Var}(\varepsilon_i) = \sigma^2_i$, then
        \[ \mathtt{Var}(\hat{\beta}) = \frac{\sum_{i=1}^{n}\sigma^2_i (x_i - \bar{x})^2}{(\sum_{j=1}^{n}(x_j - \bar{x})^2)^2} \]
        Since we can't directly estimate $\sigma^2_i$, we use the following, justified by Slutsky's
        \[  \widehat{\mathtt{Var}(\hat{\beta})} := \frac{\sum_{i=1}^{n} r_i^2 (x_i - \bar{x})^2 }{(\sum_{j=1}^{n}(x_j - \bar{x})^2)^2} \xrightarrow{p} \mathtt{Var}(\hat{\beta})\]
    \end{itemize}
  \item Independence of residuals
    \begin{itemize}
      \item Check with residual vs. fitted value plots
      \item When $\mathtt{Cov}(\varepsilon_i, \varepsilon_j) = \sigma_{ij}$, then
        \[ \mathtt{Var}(\hat{\beta}) = \frac{\sum_{i, j}\sigma_{ij}(x_i - \bar{x})(x_j - \bar{x})}{(\sum_i (x_i - \bar{x})^2)^2}\]
        The CLT still holds under weak dependence (triangular CLT).
        \[\hat{\beta} \xrightarrow{d} \mathcal{N}\left(\beta, \frac{\sigma^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\right) \]
        and
        \[\hat{\alpha} \xrightarrow{d} \mathcal{N}\left(\alpha, \sigma^2 \left(\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2} \right) \right) \]
        Point estimates are still valid, standard errors may be valid.
    \end{itemize}
\end{enumerate}

\textbf{Interpret coefficients:} ``Also the coefficient on sex is more interpretable as it directly represents on average, keeping  all other independent variables constant, the average increase/decrease in the tests scores of men compared to women.''
`On average, one would expects the post test score of two students whose pre tests scores differ by 1 point to differ by 0.7 points.'
`On average, one would expects the incumbent party’s vote share for two different elections where average recent growth in personal income in that year differ by 1 percentage point to differ by 3 percentage points.'
\subsubsection{Model diagnosis and refinement}
\begin{itemize}
  \item Autocorrelation
  \item multicollinearity - use instrumental variables
  \item Linearity and additivity violated, use log transformation - We prefer natural logs (that is, logarithms base e) because, as described above, coefficients on the natural-log scale are directly interpretable as approximate proportional differences
  \item correlated errors or latent variables to capture violations of the independence assumption, and models for varying variances and nonnormal errors.
    \begin{itemize}
      \item Use mixed effect models when there are relationships between data points (there is a known relationship in the noise). Mixed effects as a
        mix of random effects and fixed effects and are defined as
        \[ Y = X\beta + Zb + \varepsilon \]
        where $\beta$ represents $p$ fixed effects - an unknown constant that we estimate from the data and affects the mean of the response.
        $b$ represents $q$ random effects where $b \sim \mathcal{N}(0, \sigma^2_b I)$ for simplicity. Here we're mainly
        interested in estimating the parameters of the distribution, $Z$ is a correlation structure that is given to us beforehand. Random effects are convenient to address correlated observations.
        Some quantities of the marginal (marginalizing out $b$) are
        \begin{align*}
          \mathbb{E}[y | X, Z] &= X\beta\\
          \mathtt{Var}[y | X, Z] &= \mathtt{Var}[Zb] + \mathtt{Var}[\varepsilon] = \sigma^2_b Z Z^\top + \sigma^2 I =: \Sigma \\
          -2 \log \mathcal{L} &= n \log(2\pi) + \log |\Sigma| + (y - X \beta)^\top \Sigma^{-1} (y - X\beta)
        \end{align*}
        Conditional on $b$ we also have
        \begin{align*}
          \mathbb{E}[y | b] &= X \beta + Zb\\
          \mathtt{Var}[y | b] &= \mathtt{Var}[\varepsilon] = \sigma^2 I
        \end{align*}
        Estimation:
        \begin{itemize}
          \item \textbf{Estimate $\beta$: generalized least squares}

            \[
              y = X\beta + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \Sigma)
            \]

            \[
              \underbrace{\Sigma^{-1/2} y}_{\tilde{y}} = \underbrace{\Sigma^{-1/2} X}_{\tilde{X}} \beta + \underbrace{\Sigma^{-1/2} \epsilon}_{\tilde{\epsilon}}, \quad \tilde{\epsilon} \sim \mathcal{N}(0, I)
            \]

            Now applying least squares

            \[
              \hat{\beta} = (\tilde{X}^\top \tilde{X})^{-1} \tilde{X}^\top \tilde{y}
              = (X^\top \Sigma^{-1} X)^{-1} X^\top \Sigma^{-1} y
            \]

          \item \textbf{Estimate $\sigma^2$ and $\sigma_b^2$:} minimizing

            \[
              \ell \propto \log |\Sigma| + r^\top \Sigma^{-1} r
            \]
            where $r = y - X \hat{\beta}$.

          \item\textbf{Computation}
            \begin{itemize}
              \item EM algorithm
              \item Newton-Ralphson
            \end{itemize}

        \end{itemize}
    \end{itemize}
  \item Using observed data to represent a larger population, Duplicate observations, Unequal variances - Weighted regression
  \item Leverage - point furthest away from $\bar{x}$ has most leverage. Formally leverage is defined as the diagonal elements of the hat matrix
    \[l_i = H_{ii} = [X(X^\top X )^{-1} X^\top]_{ii}\]
    $l_i \in [0, 1]$. Having a high leverage implies a small residual ($r_i = \sigma^2 (1-l_i)$) and hence forces the regression line to model the point well.
    The leverage does not depend on the observed value of $y_i$ at all.
  \item Cook's distance is another diagnostic measure used to the influence of a data point and potentially identify outliers. For a
    data point $i$ is it defined as the sum of all changes in the regression model when observation $i$ is removed from it, or
    \[ D_i = \frac{\sum_{j=1}^{n} (\hat{y}_j - \hat{y}_{j(i)}^2)}{p \hat{\sigma}^2} = \frac{r_i^2}{p \hat{\sigma}^2}\left[\frac{h_{ii}}{(1 - h_{ii})^2}\right] \]
    where $p$ is the rank of the model. Takeaway: high leverage and high residual points (therefore having a large Cook's distance) are likely to be outliers.
  \item Variable selection:
    \begin{itemize}
      \item Adjusted $R^2$: adjust for model complexity
        \[ \mathtt{adj. }R^2 = 1 - \frac{SSE/(n - p -1)}{SST/(n-1)} \]
      \item Mallow's $C_p$ is another metric to see if you're overfitting
        \[C_p = SSE + 2 \hat{\sigma}p\]
      \item BIC (based on negative log likelihood assuming Gaussian noise), more consistent (small $p$ large $n$ asymptotics: if true model is $\mathcal{M}_*$ then $\mathbb{P}(\hat{\mathcal{M}} = \mathcal{M}_*) \rightarrow 1$ as $n \rightarrow \infty$ is known as consistency).
        \[BIC(\mathcal{M}) = n \log (S_{\mathcal{M}}/n) + |\mathcal{M}| \log n\]
      \item AIC (based on negative log likelihood assuming Gaussian noise), more efficient $\left(\frac{||Y - X_{\hat{\mathcal{M}}} \hat{\beta}_{\hat{\mathcal{M}}}||^2}{||Y - X_{\mathcal{M}_*} \hat{\beta}_{\mathcal{M}_*}||^2} \xrightarrow{p} 1\right)$
        \[AIC(\mathcal{M}) = n \log (S_{\mathcal{M}}/n) + 2|\mathcal{M}|\]
      \item Leave one out lemma:
        \[ CV_{\text{error}} = \sum_{i=1}^{n} (\hat{y}_i^{[-i]} - y_i)^2 = \frac{1}{n} \sum_{i=1}^{n} \left(\frac{\hat{y}_i - y_i}{1 - h_{ii}}\right)^2\]
      \item Generalized cross validation:
        \[ GCV(\mathcal{M}) = \frac{SSE_{\mathcal{M}}}{(n - |\mathcal{M}|)^2} \]
      \item $L_1$ regularization focuses on minimizing the bias squared term of MSE while $L_2$ focuses on minimizing the variance term.
    \end{itemize}
\end{itemize}
\subsubsection{Model selection/regularization}
L1/L2 regularization, use cross validation/valdiation set for model selection, Adjusted-$R^2$
\subsubsection{Notes from past problems}
\begin{itemize}
  \item Applied Qual 2024 Problem 2
    \begin{enumerate}[label=(\alph*)]
      \item Fitting a single regression line with a binary predictor between two groups and interaction term with the continuous predictor
        is equivalent to fitting two separate regression lines to the two groups since the degrees of freedom are the same and we assume noise is independent
        so fitting of one line will not affect the other.
      \item
        \begin{align*}
          \hat{\beta} &= \frac{\mathtt{cov}(x, y)}{\mathtt{var}(x)} = \rho_{x, y} \cdot \frac{s_y}{s_x}\\
          \hat{\alpha} &= \bar{y} - \hat{\beta} \bar{x}
        \end{align*}
      \item Simpson's paradox
      \item Empirical bootstrap procedure
        \begin{enumerate}[label=(\roman*)]
          \item Sample with replacement from data $n$ times
          \item Fit regression model to sampled data
          \item Repeat step i and ii $B$ times to get $\hat{\beta}_2^{(1)}, \ldots, \hat{\beta}_2^{(B)}$
          \item By asymptotic theory, we know that there exist $\sigma_j$ for $j \in [3]$, such that
            \[ \sqrt{n} (\hat{\beta}_j - \beta_j) \xrightarrow{d} \mathcal{N}(0, \sigma^2_j)\]
            Hence we can construct approximate C.I.s of the form
            \[ \hat{\beta}_j \pm z_{1 - \alpha/2} \cdot \sqrt{\widehat{\mathtt{Var}_B(\hat{\beta_j})}} \]
            where we estimate $\widehat{\mathtt{Var}_B(\hat{\beta_j})} = \frac{1}{B} \sum_{i=1}^{B} \left(\hat{\beta_j}^{(i)} - \frac{1}{B} \sum_{k=1}^{B}\hat{\beta}_j^{(k)}\right)^2$ from the bootstrap samples.
        \end{enumerate}
      \item Generally $\rho_{x, y} < 1$ (noise with non-zero variance), hence flipping will not yield same estimate.
    \end{enumerate}
  \item Applied Qual 2022 Problem A
    \begin{enumerate}[label=(\alph*)]
      \item Ablate rounding to nearest 3 months and no rounding. Then can compare following models:
        \begin{itemize}
          \item Standard OLS - issues include heteroskedastic noise (heights will vary more with age), fact that ages can't be less than 0, and would expect growth spurts so additive linear assumption is not correct.
            \[ y_i | x_i \sim \text{Normal}(\alpha + \beta x_i, \sigma^2) \]
          \item Log transformation of $y$'s (heights) remedies the second two issues from above a bit.
            \[ y_i | x_i \sim \text{LogNormal}(\alpha + \beta x_i, \sigma^2) \]
          \item Another approach could be to use a latent variable/hierarchical model, where we model the latent true age $t_i$ using a categorical latent variable $\delta_i$.
            \[ y_i | t_i \sim \text{LogNormal}(\alpha + \beta t_i, \sigma^2) \]
            \[ \delta_i \sim \text{Categorical}(\pi_1, \pi_2, \pi_3) \]
            Where $\pi_j$ for $j \in [3]$ corresponds to the probability that $\delta_i = j$ and
            \[\delta_i =
              \begin{cases}
                1 \ \ \ \text{if age is exact. Hence $t_i = x_i$.} \\
                2 \ \ \ \text{if age is rounded to nearest 6 months. Hence $t_i \in [x_i - 3, x_i + 3)$.} \\
                3 \ \ \ \text{if age is rounded to nearest 12 months. Hence $t_i \in [x_i - 6, x_i + 6)$.}
            \end{cases}\]
            For a prior on $t_i$, we assume that \[t_i \sim \text{Uniform}(0, 60)\]
            Hence the (global) parameters that we need to estimate are $\theta = \{\alpha, \beta,  \pi_1, \pi_2, \pi_3, \sigma^2 \}$.
        \end{itemize}
      \item Using second model, likelihood is just product of LogNormal pdfs
        \begin{align*}
          \mathcal{L}(\alpha, \beta, \sigma^2 | \{ x_i\}_{i=1}^{n},  \{ y_i\}_{i=1}^{n}) & = \prod_{i=1}^n \frac{1}{x_i\sqrt{2 \pi \sigma^2}} \exp\left( \frac{1}{2 \sigma^2} (\log y_i - \alpha - \beta x_i)^2 \right) \\
          &= (2 \pi \sigma^2)^{-n/2} \exp\left( \sum_{i=1}^{n} \frac{1}{2 \sigma^2} (\log y_i - \alpha - \beta x_i)^2 - \log y_i \right)
        \end{align*}
        Using the third model, the likelihood is
        \begin{align*}
        \mathcal{L}(\theta | \{ x_i\}_{i=1}^{n},  \{ y_i\}_{i=1}^{n})) &= \prod_{i=1}^{n} p(y_i, x_i | \theta) \\
        &= \prod_{i=1}^{n} \sum_{j=1}^{3} \int_{0}^{60} p(y_i, x_i, t_i, \delta_i = j|\theta) dt \\
        &= \prod_{i=1}^{n} \sum_{j=1}^{3} \int_{0}^{60} p(y_i,| t_i, \theta) p(x_i, | \delta_i = j, t_i, \theta)  p(\delta_i = j | \theta) p(t_i | \theta)dt \\
        &= \prod_{i=1}^{n} \sum_{j=1}^{3} \int_{0}^{60} \text{LogNormal}(\alpha + \beta t_i) \cdot \mathbbm{1}(\delta_{ij}(x_i)) \cdot  \pi_j \cdot \frac{1}{60} dt\\
        &= \prod_{i=1}^{n} \frac{1}{60}\Bigg( \int_{0}^{60} \text{LogNormal}(\alpha + \beta t_i) \cdot \mathbbm{1}(t_i = x_i) \cdot  \pi_1 dt \\
          &+ \int_{0}^{60} \text{LogNormal}(\alpha + \beta t_i) \cdot \mathbbm{1}(t_i \in [x_i - 3, x_i + 3)) \cdot  \pi_2 dt \\
        &+ \int_{0}^{60} \text{LogNormal}(\alpha + \beta t_i) \cdot \mathbbm{1}(t_i \in [x_i - 6, x_i + 6)) \cdot  \pi_3 dt \Bigg)  \\
        &= \prod_{i=1}^{n} \frac{1}{60}\Bigg(\text{LogNormal}(\alpha + \beta x_i)\cdot  \pi_1 + \int_{x_i - 3}^{x_i + 3} \text{LogNormal}(\alpha + \beta t_i) \cdot \pi_2 dt \\
        &+ \int_{x_i - 6}^{x_i + 6} \text{LogNormal}(\alpha + \beta t_i) \cdot  \pi_3 dt \Bigg)
      \end{align*}
      Note in the second line we marginalize over the (local) latent variables $t_i, \delta_i$.
    \item Using second model, do MLE directly. Note the MLEs for LogNormal regression is equivalent to the MLEs for OLS, except with the $y_i$'s replaced with $\log y_i$'s.
      Hence
      \begin{align*}
        \hat{\beta} &= \frac{\mathtt{Cov}(x, \log y)}{\mathtt{Var}(x)} \\
        \hat{\alpha} &= \bar{\log y} - \hat{\beta} \bar{x} \\
        \widehat{\sigma^2} &= \frac{1}{n}\sum_{i=1}^{n}(\log y_i - (\alpha + \beta x_i))^2
      \end{align*}
      Could do gradient descent as well if there are numerical issues. To do inference you could look at the predictive distribution of $\log y_i$ and invert it to get a point estimate of
      the true age. Then you could do a ``Wild'' or residual bootstrap (\href{https://faculty.washington.edu/yenchic/17Sp_403/Lec6-bootstrap_reg.pdf}{ref}) to get a distribution on the true age given a specific height maybe? (probably not right).\\

      Using the third model, you could theoretically directly maximize the log of the above observed data log likelihood via MLE. However notice that we would then have a log of a sum in addition to having to differentiate under the integral. The resulting
      expression is highly likely to run into numerical issues if you try to use it with a gradient descent type algorithm.
      As an alternative, we could do EM/MCMC/VI, for simplicity we'll just describe an EM algorithm for this model.\\

      \textbf{E step:} Compute expectations/responsibilities of latent variables using complete data log likelihood (likelihood of global parameters assuming you have observations for local latent variables). Also can be seen as estimating the posterior of the local latent variables (MAP estimate). Here the complete data log likelihood is
      \begin{align*}
        \log \mathcal{L}_C (\theta | \{ x_i\}_{i=1}^{n},  \{ y_i\}_{i=1}^{n}, \{ t_i\}_{i=1}^{n}, \{ \delta_i\}_{i=1}^{n}) &= \sum_{i=1}^{n} \log p(y_i, x_i, t_i, \delta_i | \theta) \\
        &= \sum_{i=1}^{n} \log p(y_i | t_i, \theta) + \log p(x_i | t_i, \delta_i, \theta) + \log p(t_i) + \log p(\delta_i | \theta)\\
        &\propto \sum_{i=1}^{n} \left(-\frac{1}{2} \log \sigma^2 + \frac{1}{2 \sigma^2} (\log y_i - \alpha - \beta t_i)^2 + \sum_{j=1}^{3}\mathbbm{1}(\delta_i = j)\log \pi_j \right)
      \end{align*}
      where we drop terms that do not depend on $\theta$ (i.e. $\log p(x_i | t_i, \delta_i, \theta)$ and $\log p(t_i)$).
      Then for the E step, given an initial guess for $\theta^{(0)}$, we compute
      \begin{align*}
        \mathbb{E}_{\mathbf{t}, \mathbf{\delta} | \mathbf{x}, \mathbf{y}, \theta^{(0)}}[\log \mathcal{L}_C (\theta | \phi)] &\propto \mathbb{E}_{\mathbf{t}, \mathbf{\delta} | \mathbf{x}, \mathbf{y}, \theta^{(0)}}\left[\sum_{i=1}^{n} \left(-\frac{1}{2} \log \sigma^2 + \frac{1}{2 \sigma^2} (\log y_i - \alpha - \beta t_i)^2 + \sum_{j=1}^{3}\mathbbm{1}(\delta_i = j)\log \pi_j \right)\right]
      \end{align*}
      where we use $\phi$ as a shorthand for the complete data ($\{ x_i\}_{i=1}^{n},  \{ y_i\}_{i=1}^{n}, \{ t_i\}_{i=1}^{n}, \{ \delta_i\}_{i=1}^{n}$). Inspecting the above expression,
      we notice we need to compute three expressions:
      \begin{enumerate}[label=(\arabic*)]
        \item
          \begin{align*}
            \mathbb{E}_{\mathbf{t}, \mathbf{\delta} | \mathbf{x}, \mathbf{y}, \theta^{(0)}}[\mathbbm{1}(\delta_i = j)] &= \sum_{\delta=1}^{3} \int \mathbbm{1}(\delta_i = j) p(\mathbf{t}, \mathbf{\delta} | \mathbf{x}, \mathbf{y}, \theta^{(0)}) d\mathbf{t}  \\
            &= \sum_{\delta=1}^{3} \int \mathbbm{1}(\delta_i = j) p(\mathbf{t}  | \mathbf{\delta}, \mathbf{x}, \mathbf{y}, \theta^{(0)}) p(\mathbf{\delta} | \mathbf{x}, \mathbf{y}, \theta^{(0)})d\mathbf{t} \\
            &= p(\delta_i = j | \mathbf{x}, \mathbf{y}, \theta^{(0)}) \int  p(\mathbf{t}  | \mathbf{\delta}, \mathbf{x}, \mathbf{y}, \theta^{(0)}) d\mathbf{t}\\
            &= p(\delta_i = j | \mathbf{x}, \mathbf{y}, \theta^{(0)})
          \end{align*}
          where above we use the chain rule. To actually compute this posterior probability, we appeal to Bayes rule:
          \begin{align*}
            p(\delta_i = j | \mathbf{x}, \mathbf{y}, \theta^{(0)}) &= \frac{p(\delta_i = j,  \mathbf{x}, \mathbf{y} | \theta^{(0)})}{p(\mathbf{x}, \mathbf{y} | \theta^{(0)})} \\
            &= \frac{p( \mathbf{x}, \mathbf{y} |\delta_i = j,  \theta^{(0)}) p(\delta_i = j | \theta^{(0)})}{\sum_{k=1}^{3}p(\mathbf{x}, \mathbf{y}, \delta_i = k | \theta^{(0)})} \\
            &= \frac{p( \mathbf{x}, \mathbf{y} |\delta_i = j,  \theta^{(0)}) \pi_j}{\sum_{k=1}^{3}p(\mathbf{x}, \mathbf{y}| \delta_i = k, \theta^{(0)}) \pi_k}
          \end{align*}
          where as above in the likelihood part
          \begin{align*}
            p( \mathbf{x}, \mathbf{y} |\delta_i = 1,  \theta^{(0)}) &= \int_{0}^{60} p( \mathbf{x}, \mathbf{y}, \mathbf{t} |\delta_i = 1,  \theta^{(0)}) d\mathbf{t} = \int_{0}^{60} p( \mathbf{x} | \mathbf{t}, \delta_i = 1,  \theta^{(0)}) p( \mathbf{y} | \mathbf{t}, \delta_i = 1,  \theta^{(0)}) p( \mathbf{t})  d\mathbf{t} \\
            &=  \text{LogNormal}(\alpha + \beta x_i) \cdot \frac{1}{60} \\
            \\
            p( \mathbf{x}, \mathbf{y} |\delta_i = 2,  \theta^{(0)}) &= \int_{0}^{60} p( \mathbf{x}, \mathbf{y}, \mathbf{t} |\delta_i = 1,  \theta^{(0)}) d\mathbf{t} = \int_{0}^{60} p( \mathbf{x} | \mathbf{t}, \delta_i = 1,  \theta^{(0)}) p( \mathbf{y} | \mathbf{t}, \delta_i = 1,  \theta^{(0)}) p( \mathbf{t})  d\mathbf{t} \\
            &=  \int_{\mathbf{x} - 3}^{\mathbf{x} + 3}\text{LogNormal}(\alpha + \beta \mathbf{t}) \cdot \frac{1}{60} d\mathbf{t} \\
            \\
            p( \mathbf{x}, \mathbf{y} |\delta_i = 3,  \theta^{(0)}) &= \int_{0}^{60} p( \mathbf{x}, \mathbf{y}, \mathbf{t} |\delta_i = 1,  \theta^{(0)}) d\mathbf{t} = \int_{0}^{60} p( \mathbf{x} | \mathbf{t}, \delta_i = 1,  \theta^{(0)}) p( \mathbf{y} | \mathbf{t}, \delta_i = 1,  \theta^{(0)}) p( \mathbf{t})  d\mathbf{t} \\
            &=  \int_{\mathbf{x} - 6}^{\mathbf{x} + 6}\text{LogNormal}(\alpha + \beta \mathbf{t}) \cdot \frac{1}{60} d\mathbf{t} \\
          \end{align*}
        \item
          \begin{align*}
            \mathbb{E}_{\mathbf{t}, \mathbf{\delta} | \mathbf{x}, \mathbf{y}, \theta^{(0)}}[t_i] &= \sum_{k=1}^{3}\int \mathbf{t} \cdot p(\mathbf{t}, \delta = k| \mathbf{x}, \mathbf{y}, \theta^{(0)}) d\mathbf{t}\\
            &= \sum_{k=1}^{3}\int \mathbf{t} \cdot p(\mathbf{t}| \delta = k, \mathbf{x}, \mathbf{y}, \theta^{(0)}) p(\delta = k | \mathbf{x}, \mathbf{y}, \theta^{(0)}) d\mathbf{t}
          \end{align*}
          where again by Bayes rule
          \begin{align*}
            p(\mathbf{t}| \delta = k, \mathbf{x}, \mathbf{y}, \theta^{(0)}) &= \frac{p(\delta = k, \mathbf{x}, \mathbf{y}, \mathbf{t} | \theta^{(0)})}{p(\delta = k, \mathbf{x}, \mathbf{y} | \theta^{(0)})} \\
            &= \frac{p(\delta = k | \theta^{(0)}) p( \mathbf{x}, \mathbf{y}, \mathbf{t} | \delta = k, \theta^{(0)})}{p(\delta = k| \theta^{(0)}) p( \mathbf{x}, \mathbf{y} | \delta=k, \theta^{(0)})} \\
            &= \frac{\text{LogNormal}(\alpha + \beta t_i) \cdot \mathbbm{1}(\delta_{ik}(x_i))\cdot \frac{1}{60}}{\int_{0}^{60} p( \mathbf{x} | \mathbf{t}, \delta_i = k,  \theta^{(0)}) p( \mathbf{y} | \mathbf{t}, \delta_i = k,  \theta^{(0)}) p( \mathbf{t})  d\mathbf{t}}
          \end{align*}
          where hence we have already described how to calculate all of the above quantities.
        \item Likewise
          $$\mathbb{E}_{\mathbf{t}, \mathbf{\delta} | \mathbf{x}, \mathbf{y}, \theta^{(0)}}[t_i^2] = \sum_{k=1}^{3}\int \mathbf{t}^2 \cdot p(\mathbf{t}| \delta = k, \mathbf{x}, \mathbf{y}, \theta^{(0)}) p(\delta = k | \mathbf{x}, \mathbf{y}, \theta^{(0)})d\mathbf{t} $$
      \end{enumerate}

      \textbf{M step:} Maximize expected value of the complete data log likelihood to estimate global parameters $\theta$, analogous to MLE.
      Update $\pi_k$ by taking average responsibility over data.
      $$Q_{\pi}(\pi) = \sum_{i=1}^n E \left[ \sum_{k=1}^3 I(z_i=k) \log(\pi_k) \right] = \sum_{i=1}^n \sum_{k=1}^3 E[I(z_i=k)] \log(\pi_k)$$Since $E[I(z_i=k)] = p(z_i=k | a_i, h_i, \theta^{(j)}) = w_{ik}^{(j)}$, this simplifies to:$$Q_{\pi}(\pi) = \sum_{i=1}^n \sum_{k=1}^3 w_{ik}^{(j)} \log(\pi_k)$$We need to maximize this function subject to the constraint that $\sum_{k=1}^3 \pi_k = 1$. We use a Lagrange multiplier, $\lambda$.$$\mathcal{L}(\pi, \lambda) = \sum_{i=1}^n \sum_{k=1}^3 w_{ik}^{(j)} \log(\pi_k) + \lambda(1 - \sum_{k=1}^3 \pi_k)$$Taking the derivative with respect to $\pi_k$ and setting it to zero:$$\frac{\partial \mathcal{L}}{\partial \pi_k} = \sum_{i=1}^n \frac{w_{ik}^{(j)}}{\pi_k} - \lambda = 0 \implies \pi_k = \frac{\sum_{i=1}^n w_{ik}^{(j)}}{\lambda}$$To find $\lambda$, we sum over all $k$ and use the constraint:$$\sum_{k=1}^3 \pi_k = 1 \implies \frac{1}{\lambda} \sum_{k=1}^3 \sum_{i=1}^n w_{ik}^{(j)} = 1$$The sum $\sum_{k=1}^3 \sum_{i=1}^n w_{ik}^{(j)} = \sum_{i=1}^n \sum_{k=1}^3 w_{ik}^{(j)}$. Since $\sum_{k=1}^3 w_{ik}^{(j)} = 1$ for any child $i$, the total sum is simply $n$.$$\frac{n}{\lambda} = 1 \implies \lambda = n$$

      Update $\alpha, \beta, \sigma^2$ by doing a weighted least squares regression, where we use the values for $\mathbb{E}_{\mathbf{t}, \mathbf{\delta} | \mathbf{x}, \mathbf{y}, \theta^{(0)}}[t_i]$ and
      $\mathbb{E}_{\mathbf{t}, \mathbf{\delta} | \mathbf{x}, \mathbf{y}, \theta^{(0)}}[t_i^2]$ that we calculated in the E step above in place of $t_i$ and $t_i^2$. \\

      To do inference on the true age of a child, we calculate
      \[p(t_i| \mathbf{x}, \mathbf{y}, \hat{\theta}) = \sum_{k=1}^{3} p(t_i| \delta=k, \mathbf{x}, \mathbf{y}, \hat{\theta}) p(\delta=k| \mathbf{x}, \mathbf{y}, \hat{\theta})  \]
      From this probability distribution we can calculate a point estimate and look at the quantiles if it is a nice distribution or else do an empirical bootstrap.

    \item Applied Qual 2021 Question 6
      \begin{enumerate}[label=(\arabic*)]
        \item (Why isn't the intercept zero?) This is an artifact from fitting the least squares criteria. Since the linear model is an approximation to a potentially non linear relationship, there may be measurement noise, and
          there maybe a lack of data points of trees near $x=0$, hence the model is extrapolating and in order to minimize the sum of squared errors this would result in the intercept being a non zero value.
        \item See written notes: main takeaway is for KNN note what kinds of points are near a query/test point.
        \item See written notes.
      \end{enumerate}
    \item Applied Qual 2004 Question 4
      \begin{enumerate}[label=(\arabic*)]
        \item Advantages to putting all points on edges of range: minimize variance of regression slope coefficient estimate. Disadvantage: if relationship is not perfectly linear the estimates will be very poor, cannot check for non linearity.
        \item Uniformly sample the range and do residual bootstrap to check the quantile of the $F$-ratio value comparing a quadratic model with a linear model. Could also look to see if slope estimate is 0 ($t$-test) or equivalently the $F$ test.
      \end{enumerate}
    \item Applied Qual 2019 Question B
      \begin{enumerate}[label=(\arabic*)]
        \item LDA (assumptions are directly met), LogReg (linear decision boundary), QDA (normality assumption correct but too flexible, causes variance term of MSE to go up), KNN (too flexible again, variance goes up with no reduction in bias)
        \item QDA (assumptions are directly met), KNN (underlying decision boundary could be quadratic), LDA (normality assumption correct, but incurs high bias), LogReg (also incurs high bias)
        \item KNN (bias term of MSE goes down due to flexibility), QDA (more flexible decision boundary), LogReg/LDA
      \end{enumerate}
    \item Applied Qual 2018 Question 5
      \begin{itemize}
        \item LDA discriminant function equation
          \[ \delta_k (x) = x^\top \Sigma^{-1}\hat{\mu}_k  - \frac{1}{2} \hat{\mu}_k^\top \hat{\Sigma}^{-1} \hat{\mu}_k + \log \hat{\pi}_k \]
          Logistic regression handles unbalanced classes better than LDA.
      \end{itemize}
    \item Applied Qual 2019 Question D
      \begin{itemize}
        \item Bias-variance tradeoff: as you increase model complexity your bias decreases generally but your variance will generally increase.
        \item If you know the Bayes error rate, then when the training error rate goes below the Bayes error rate, that is when overfitting begins/clear symptom of overfitting. Where the test error starts increasing is when overfitting starts to have a negative impact on the model's usefulness, and is conventionally considered the point where overfitting begins.
      \end{itemize}
    \item Applied Qual 2014 Question 3
      \begin{itemize}
        \item If there's no systemic error, allocating points where there is the most measurement variance results in the best regression coefficient estimates. Assign 1/3 of points to 1 and 2/3 of points to 3 since assigning any points to the middle does not help at all.
        \item Weighted least squares setup: want to min $\sum_{i=1}^{n}w_i(y_i - X_i \beta)^2$ for non negative weights $w_1, \ldots, w_n$. Then the regression coefficient estimates are
          \[ \hat{\beta} = (X^\top W X)^{-1} X^\top W Y \]
          Equivalently for simple regression
          \begin{align*}
            \hat{\alpha} &= \frac{1}{\sum_{i=1}^{n}w_i} \sum_{i=1}^{n}w_i y_i - \hat{\beta} \frac{1}{\sum_{i=1}^{n}w_i} \sum_{i=1}^{n}w_i x_i = \bar{y}_w - \hat{\beta} \bar{x}_w\\
            \hat{\beta} &= \frac{\sum_{i=1}^{n} w_i (x_i - \bar{x}_w)(y_i - \bar{y}_w)}{\sum_{i=1}^{n} w_i (x_i - \bar{x}_w)^2}
          \end{align*}
      \end{itemize}
    \item Applied Qual 2014 Question 4
      \begin{itemize}
        \item $p$-value is distributed for the null hypothesis and the assumptions are met. Some violations include the true distribution of the data not following the data (ex. regression assumptions not being met, so $t$-dist might not be right), or when the test statistic is discrete (like a $\chi^2$ test). When the model assumptions are violated, the calculated p-values will not be uniformly distributed under the null. If the data has heavier tails than a normal distribution, for example, you might get more extreme test statistics than expected, leading to an excess of small p-values and an inflated Type I error rate (i.e., you reject the null hypothesis more often than the significance level $\alpha$ suggests you should).
      \end{itemize}
    \item Applied Qual 2020 Question B
      \begin{itemize}
        \item For categorical/nominal data you can use a $\chi^2$ goodness of fit test. Here the test statistic is
          \[ \sum_{i=1}^{n} \frac{(\text{expected} - \text{observed})^2}{\text{expected}} \sim \chi^2_{n-1} \]
          We can then check the $p$-value for level $\alpha=0.05$ and reject the null that the data is drawn from a Poisson distribution with mean 11.6. The procedure would lead to the conclusion that there is statistically significant evidence of overdispersion. The discrepancy is too large to be attributed to random chance from a small sample, which contradicts Professor Jones's initial suggestion and supports his assertion that the model is not appropriate.
        \item The problem likely is that each count is Poisson but not identically distributed. That is we would expect the mean parameter to be different for each national park.
          One solution is to use a negative binomial model to the counts, which allows for more variance compared to a Poisson distribution. Another more sophisticated model could be to use a Bayesian hierarchical model,
          where the lambda for each park is drawn from a Gamma prior. This Gamma-Poisson mixture model is mathematically equivalent to the Negative Binomial model. Its advantage is that it directly models the source of the overdispersion (the variation in the lambdas). Jane could then estimate the average salamander count across all parks (expected value of lambda) and also quantify the variation between parks (variance of lambda), providing a richer answer about the "spread" of the salamander population.
          You can fit alpha and beta for the Gamma distribution via an Empirical Bayes procedure, like method of moments, or do MLE with posterior distribution since it's conjugate.
      \end{itemize}
    \item Applied Qual 2018 Question 3
      \begin{itemize}
        \item Did not correct for multiple testing (genetic variants are not independent of each other and are in fact highly correlated). One way
          to correct for this is to do PCA beforehand to denoise highly correlated variants, and then run Lasso Regression to see which components are predictive in
          of Biasphobia. Finally we can check the loadings to see which genetic variants play a role in predicting Biasphobia.
      \end{itemize}
  \end{enumerate}

\end{itemize}

Regression dilution, also known as regression attenuation, is the biasing of the linear regression slope towards zero (the underestimation of its absolute value), caused by errors in the independent variable.

\subsection{Experimental Design}
Mostly notes and good equations to know from ROS.

\subsubsection{Standard errors and confidence intervals}
\textbf{Averages and proportions}: standard error - $\sqrt{\hat{p}(1-\hat{p})/n}$ (normal approximation to binomial distribution), acceptable when $n - y$ and $y$ are both greater than 5.
if not we standard correction is $\hat{p} = \frac{y + 2}{n+4}$ with standard error $\sqrt{\hat{p}(1 - \hat{p})/(n+4)}$ while truncating the confidence intervals to make sense.
A ``good chance'' of distinguishing between two proportions can be interpreted as having the difference be equal to the standard error (or even 2 times so that a 95\% confidence interval will barely exclude zero).
\\

\textbf{Comparisons}: standard error of a difference - $\sqrt{se_1^2 + se_2^2}$ \\

\textbf{Weighted average}: average proportion - $\frac{N_1}{N_{tot}}\hat{p}_1 + \frac{N_2}{N_{tot}}\hat{p}_2 + \ldots$, standard error - $\sqrt{(\frac{N_1}{N_{tot}} se_1)^2 + (\frac{N_2}{N_{tot}}se_2)^2 + \ldots}$\\

\textbf{Scaling sample size}: In general, by increasing the sample size by $N$ times you decrease the standard error by $1/\sqrt{N}$ times.

\subsubsection{Properties of regression}
\begin{itemize}
\item Regression with just an intercept results in predicting the sample mean of the $y$'s
\item Regression with a binary indicator variable results in the coefficient estimate being the difference in the means of response variable for the 2 cases of the indicator variable, and the
  standard error being the sqrt of the sum of the standard errors of each group.
\end{itemize}

\subsubsection{Sample size/power calculations}
\textbf{For distinguishing proportions}: If the goal is 80\% power to distinguish $p$ from a specified value $p_0$, then a conservative required sample size is that
needed for the true parameter value to be 2.8 standard errors from zero; solving for this standard error
yields $n = (2.8 \cdot 0.5/(p - p_0))^2$ or, more precisely, $n = p(1 - p)(2.8/(p - p_0))^2$\\

\textbf{Comparison of proportions, equal sample sizes}: The standard error of a difference between two proportions is, by a simple probability calculation, $\sqrt{p_1 (1 - p_1)/n_1 + p_2 (1 - p_2)/n_2}$, which has an upper bound of $0.5\sqrt{ 1/n_1 + 1/n_2}$. If we assume
$n_1 = n_2 = n/2$ (equal sample sizes in the two groups), the upper bound on the standard error becomes
simply $1/\sqrt{n}$. A specified standard error can then be attained with a sample size of $n = 1/(s.e.)^2$. If
the goal is 80\% power to distinguish between hypothesized proportions $p_1$ and $p_2$ with a study of size
$n$, equally divided between the two groups, a conservative sample size is $n = ((2.8/(p_1 - p_2))^2$ or,
more precisely, $n = 2(p_1 (1 - p_1) + p_2 (1 - p_2))(2.8/(p_1 - p_2))^2$. \\

\textbf{Estimates of means}: If the goal is 80\% power to distinguish $\theta$ from a
specified value $\theta_0$, then a conservative required sample size is $n = (2.8 \sigma/(\theta - \theta_0))^2$.

\subsubsection{Missing data imputations/casual inference}
Complete case analysis is where you throw out any data points with any missing attributes. Available case is where you throw out data that have missing attributes you're interested in studying.
The Sample Average Treatment Effect (SATE) is the average of the individual treatment effects for all units in the sample,
A simple regression of the observed outcome $y$ on the treatment indicator $z$ estimates the average treatment effect as the difference in the mean observed outcomes between the treated group and the control group.
Using deterministic imputations results in too many values in the middle of the distribution.
\subsection{Logistic regression}
\subsubsection{Some properties}
\textbf{Scaling and shifting the sigmoid/inverse logit function}: Scaling $x$ will cause the curve in the middle to sharpen/flatten. Shifting moves the midpoint ($y=0.5$) in the opposite direction of the shift away from $x=0$ by $a/b$ units.
Can also find the midpoint by setting $a + bx = 0$\\

\textbf{Divide by 4 rule}: ``From the divide by 4 rule, near the 50\% point, comparing two people who differ by one inch in height, you’ll expect a difference of 0.28/4 = 0.07 in the probability of being heavy.'' This is an upper bound on the predictive difference corresponding to a unit difference in $x$.\\

\textbf{Log-odds}: \[ \log \frac{\mathbb{P}(y = 1 | X)}{\mathbb{P}(y = 0 | X)} = \alpha + \beta X \]
\subsubsection{Model assumptions}
\subsubsection{Estimation}
% \subsubsection{Inference questions}
% \subsubsection{Model diagnosis and refinement}
% \subsubsection{Model selection/regularization}

% \subsection{Non-parametric models}
% \subsubsection{Model assumptions}
% \subsubsection{Estimation}
% \subsubsection{Inference questions}
% \subsubsection{Model diagnosis and refinement}
% \subsubsection{Model selection/regularization}

% \subsection{Models with latent components including mixed-effect/multilevel models, factor models, etc.}
% \subsubsection{Model assumptions}
% \subsubsection{Estimation}
% \subsubsection{Inference questions}
% \subsubsection{Model diagnosis and refinement}
% \subsubsection{Model selection/regularization}

\section{Bayesian Data Analysis}
Applied and computational Bayesian statistics\\

Good to know: normal-normal posterior distribution with known variance
\[ \mathcal{N}\left(\frac{\left( \frac{\mu_0}{\sigma_0^2} + \frac{\sum_{i=1}^{n}x_i}{\sigma^2} \right)^2}{\frac{n}{\sigma^2} + \frac{1}{\sigma_0^2}}, \left(\frac{n}{\sigma^2} + \frac{1}{\sigma_0^2}\right)^{-1}\right)\]
% \subsection{Bayesian Hierarchical Modeling}
\subsection{Fake-data simulation to design an experiment}
% \subsection{Modeling using splines/Gaussian processes}
% \subsection{Computational workflow}

\section{Statistical Machine Learning}
\subsection{Linear and nonlinear dimensionality reduction}
Good to memorize: Gaussian conditioning formula
\begin{align*}
  \begin{pmatrix}x_{1}\\
    x_{2}
  \end{pmatrix} &\sim  \mathcal{N}
  \begin{pmatrix}
    \begin{bmatrix}
      \mu_1\\
      \mu_2
    \end{bmatrix}\!\!,&
    \begin{bmatrix}
      \Sigma_{11} & \Sigma_{12}\\
      \Sigma_{21} & \Sigma_{22}
    \end{bmatrix}
  \end{pmatrix}\\[2\jot]
  x_1 | x_2 &\sim \mathcal{N}(\mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(x_2 - \mu_2), \Sigma_{11} - \Sigma_{12}\Sigma^{-1}_{22} \Sigma_{21})
\end{align*}

Woodbury matrix identity
\[ (A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1} \]

Matrix calculus rules:
\begin{align*}
  \frac{\partial \log \det (A)}{\partial A} &= A^{-\top} \\
  \frac{\partial \text{Tr}(AX^{-1}B) }{\partial X} &= -(X^{-1}BAX^{-1})^\top \\
  \frac{\partial \beta^{\top} \Sigma \beta^{\top}}{\partial \beta} &= (\Sigma + \Sigma^\top)\beta
\end{align*}

PPCA equations to know:
\begin{align*}
  \text{(from PPCA paper)} \  \frac{\partial L}{\partial \Lambda} &= N (C^{-1}S C^{-1}\Lambda - C^{-1}\Lambda)\\
  \Lambda &= SC^{-1} \Lambda \ \text{(for non trivial solutions consider SVD of $\Lambda$)} \\
  &\text{For MLE of $\sigma^2$ consider eigendecomposition of S and note } C = UD'U^T
\end{align*}

Kernel PCA reconstruction - need to use eigenvectors of kernel Gram matrix - $z_{ik} = \sum_{j=1}^n \alpha_{kj} K(X_j, X_i)$ \\

Eckart–Young–Mirsky theorem: Best rank $k$ approximation of a matrix in the spectral norm is the SVD.
\subsection{Data-driven and model-based classification methods}
Bayes optimal classifier procedure - estimate conditional probability $P(Y=k|X)$ via Bayes rule and plugin estimate.

\subsection{Data-driven and model-based clustering methods}
For $K$-means type algorithms note that $L(z^{(i)}, \Sigma^{(i)}) \leq L(z^{(i)}, \Sigma^{(i-1)}) \leq L(z^{(i-1)}, \Sigma^{(i-1)})$ since loss is convex.
\subsection{Graphical models: Bayesian networks, Markov random fields}
Graphs factor by cliques \\
Bayes Ball algorithm for determining if there are conditional independence relationships in a DAG: a path is blocked if it
contains an observed (part of conditioning set) non-collider node (like a chain or fork) or a collider node that is not observed and none of its descendants are observed.

\subsection{Latent variable models}
Summation inside the logs (e.g. mixture models) breaks concavity in log likelihoods and thus requires us to use EM type algorithms.\\
\subsubsection{VI}
If $z_n$ is drawn from a categorical distribution then
\[ q(z) \propto \prod_{i=1}^{N}\prod_{k=1}^{K} (e^{\rho_{nk}})^{z_{nk}}\]
Then $q(z_nk = 1) = \frac{\rho_{nk}}{\sum_{k'=1}^{K}\rho_{nk'}}$
\subsection{Introduction to Deep Learning: Deep generative models, Approximate inference}

\section{Computation}
\subsection{Gradient-based optimization methods}
\subsection{Monte Carlo methods: sampling from univariate and multivariate distributions}

% \begin{enumerate}
%     \item {\it Open-Ended - Mathematical \& Pseudocode}.  Consider fitting logistic regression with inputs $\mathbf{x}_i \in \mathcal{R}^p$ and $y_i \in \{0,1\}$ for $i = 1, \ldots n$.
%     \begin{itemize}
%         \item Write out the cross-entropy loss (Bernoulli maximum likelihood estimation) problem for the logistic regression model.
%         \item Compute the gradient of this loss with respect to the parameters.
%         \item In pseudocode, write out a function to perform gradient descent to fit this logistic regression model.  Your function should take inputs $\mathbf{X}$, $Y$, {\tt learning rate}, and {\tt max.iterations}.
%         \item Discussion: Is your gradient descent function a good way to fit logistic regression?  Why or why not?  If not, suggest other strategies used in machine learning to optimize this loss.
%     \end{itemize}

%     \textbf{Answer:}

% $$\ell(\mathbf \beta \mid \mathbf X, \mathbf y)
%   \;=\;
%   -\sum_{i=1}^n \Bigl[
%       y_i \,\log \sigma(\mathbf x_i^\top \mathbf{\beta}
%       )
%       \;+\;
%       (1 - y_i)\,\log\bigl(1 - \sigma(\mathbf x_i^\top \mathbf \beta)\bigr)
%     \Bigr] = - Y \cdot \mathbf{X}\beta + \log(1 + e^{\mathbf{X}\beta})$$
% where $\sigma(z)=\frac{1}{1+e^{-z}} = \frac{e^z}{1 + e^z}$.
% $$\frac{\partial \ell}{\partial \beta}
% \;=\;
% -\sum_{i=1}^n
%   \Bigl[
%     y_i - \sigma(\mathbf x_i^\top \mathbf \beta)
%   \Bigr]\,
%   \mathbf x_i
% \;=\;
% \mathbf X^\top \bigl(\sigma(\mathbf X\mathbf \beta)-Y\bigr)$$
% where $\sigma(\mathbf X\mathbf \beta) = \bigl[\sigma(\mathbf x_1^\top\beta),\dots,\sigma(\mathbf x_n^\top \beta)\bigr]^\top$
% \\

% \begin{algorithmic}[1]
% \Function{LogisticGD}{$\mathbf X,\, Y,\,\mathtt{learning\_rate}, \mathtt{max.iterations}$}
%   \State Initialize $\mathbf w\gets \mathbf 0\in\mathbb R^p$
%   \State $\mathtt{tol} \gets 1\mathtt{e}-4$
%   \State $\mathtt{prev\_log\_likelihood} \gets - Y \cdot \mathbf{X}\beta + \log(1 + e^{\mathbf{X}\beta})$
%   \For{$t=1$ to $\mathtt{max.iterations}$}
%     \State $\mathbf p \gets \sigma(\mathbf X\,\mathbf w)$
%     \State $\mathbf g \gets \mathbf X^\top(\mathbf p - Y)$
%     \State $\mathbf w \gets \mathbf w - \mathtt{learning\_rate} \cdot\mathbf g$
%     \State $\mathtt{curr\_log\_likelihood} \gets - Y \cdot \mathbf{X}\beta + \log(1 + e^{\mathbf{X}\beta})$
%     \If{$\mathtt{prev\_log\_likelihood} - \mathtt{curr\_log\_likelihood} < \mathtt{tol}$}
%         \State \textbf{break}
%     \EndIf
%     \State $\mathtt{prev\_log\_likelihood} \gets \mathtt{curr\_log\_likelihood}$
%   \EndFor
%   \State \Return $\mathbf w$
% \EndFunction
% \end{algorithmic}
% While simple and easy to implement, gradient descent can be \emph{slow} to converge on large datasets:
% \begin{itemize}
%   \item It requires computing the gradient over all $n$ samples at each iteration.
%   \item It can get stuck in plateaus or require very small step‐sizes (learning rates) for stability.
% \end{itemize}
% \quad\textit{Alternative optimization methods:}
% \begin{itemize}
%   \item \textbf{Newton–Raphson / IRLS}: uses second‐order information (Hessian) for quadratic convergence.
% \end{itemize}

% \item  {\it Open-ended - Conceptual}. Outliers are defined as observations that are aberrant.  (i) Which dimension reduction method(s)
% would be best for finding outliers
% in a data set?  Which clustering method(s)?  Why?
% (ii) If you knew a data set might contain outliers or other aberrations, which families of supervised learning methods would you recommend that would be robust to these outliers? Why?

%     \textbf{Answer:} In terms of dimension reduction methods:
%     \begin{itemize}
%         \item PCA - maximizes variance between points which might make it easier to visualize
%         \item MDS - preserves distances, big distances should be preserved
%         \item UMAP/t-SNE/LLE/Isomap - non linear dim reduction methods push far points away from each other
%     \end{itemize}
%     In terms of clustering methods:
%     \begin{itemize}
%         \item Single linkage clustering - able to find outliers in the dendrogram
%         % \item maybe k-means with large enough k?
%         \item spectral clustering with connected components
%     \end{itemize}

%     In terms of supervised learning methods robust to outliers:
%     \begin{itemize}
%         \item Gradient boosting with L1 loss, not L2
%         \item Random forests and general ensemble based methods - handful of weak learners won't skew overall global prediction
%     \end{itemize}
% \item {\it Open-ended - Conceptual}. (i) List all of the types of techniques that are commonly employed in machine learning to help prevent overfitting.
% (ii) For each technique in part (i), discuss why this technique is useful for preventing overfitting.
% (iii) For each technique in part (i), list some machine learning models that use this technique.

%     \textbf{Answer:}

%     \begin{itemize}
%         \item Regularization (L2/L1) - prevent weights from getting too big, too complex of a function, linear regression
%         \item Dropout, bagging - lower variance predictions, NN/RF
%         \item Early stopping - NN, stop training to prevent overfitting, avoid bad local optimal
%         \item Data augmentation - NN, force model to learn robust decision rules, avoid bad local optimal
%         \item Small learning rate (slow learning) - Gradient boosting, don't fit optimal all at once which hopefully prevents overfitting
%         \item Stochastic gradients  - NNs, add noise for implicit regularization
%     \end{itemize}

% \item  {\it Open-ended - Conceptual}.  Suppose you are building a binary classifier with unequal consequences for misclassification errors (e.g. incorrectly predicting something as class +1 is much worse than incorrectly predicting something as class -1).  Recommend a way to handle this situation when using (i) logistic regression, (ii) support vector machines, (iii) random forests, or (iv) a deep neural network.

%     \textbf{Answer:}
%     \begin{itemize}
%         \item ROC curve with all of them to decide decision threshold, but especially for logistic
%         \item Modify slack variable to be class specific
%         \item When fitting each tree, weight each training example. This effectively biases the impurity measure (e.g.\ Gini) to focus on the class that carries higher cost. In sklearn you can pass $sample\_weight$ per example or $class\_weight$ per class.
%         \item Weighted cross entropy loss with all of them, especially NN
%     \end{itemize}

% \item {\it Open-ended - Conceptual}.  Consider two feedforward neural network models: Model P consists of a single hidden layer with 128 units.  Model Q consists of 2 hidden layers, the first with 128 units and the second with 8 hidden units.  Both models use identical activations.  (i) What are the comparative advantages and disadvantages of Model P verses Model Q?  (ii) Give an example where you would want to use each model.

%     \textbf{Answer:} Second one is deeper, can learn more hierarchical/compositional feature, has less parameters if output dimension is small. First one has less information bottleneck (could be more prone to overfitting), could be easier to train with less issues of vanishing gradients. 1st one 50 class classification, or when you think all the features contributes to the output in a roughly additive or smooth way (house-price prediction from numeric/one-hot features, they aren't compositional). Second one regression (single dimension output), or when you're trying to learn a latent representation (non linear dimension reduction, you think your data lies on a low dimensional manifold, like MNIST).

% \end{enumerate}

\end{document}
